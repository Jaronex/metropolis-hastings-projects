{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "metropolis_hastings_theory",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyPD1hmGCWf1oUZSxyo6tFNH",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Jaronex/metropolis-hastings-projects/blob/main/metropolis_hastings_theory.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Introduction\n",
        "\n",
        "This notebook is a code-along to *A Practical Guide to MCMC Part 1: MCMC Basics* by Justin Ellis. The tutorial can be found [here](https://jellis18.github.io/post/2018-01-02-mcmc-part1/). My goal is to understand the underlying mechanisms behind Metropolis-Hastings sampling and eventually integrate it into more sophisticated models e.g. [MHGAN](https://arxiv.org/pdf/1811.11357.pdf), [PMH](https://arxiv.org/pdf/1511.01707.pdf) etc.\n",
        "\n",
        "MCMCs are used to map out and sample probability distributions/density functions. The following apply:\n",
        "\n",
        "1.   MCMCs explore parameter space such that histogram of samples produce target distribution\n",
        "2.   Markovian: Movement from one state to the next only depends on current position and transition probability i.e. conditionally independent of prior states\n",
        "3.   Chain will converge to the target distribution if transition probability is *irreducible*, *positive recurrent* and *aperiodic* \n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "TkShe3eILH0b"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Metropolis-Hastings MCMC\n",
        "\n",
        "In essence, the Metropolis-Hastings algorithm accepts a \"jump\" from a point in a parameter space $x_i$ to $x_{i+1}$ with the following probability:\n",
        "\n",
        "$$ κ(x_{i+1}|x_i) = min(1, \\frac{π(x_{i+1})q(x_i|x_{i+1})}{π(x_i)q(x_{i+1}|x_i)}) $$\n",
        "\n",
        "where the fraction above the Hastings ratio, $H$. $H$ is the ratio of the new posterior distribution and transition probability value to the old posterior distribution and transition probability value. If $H > 1$, jump is accepted.\n",
        "\n",
        "After initialising parameters $x_0$,the following steps can be taken to implement the algorithm $\\forall\\;i = {1, 2,...,n}$:\n",
        "\n",
        "\n",
        "1.   Generate proposed parameters: $x_* \\sim q(x_*|x_i)$\n",
        "2.   Sample from uniform distribution $u \\sim U(0,1)$\n",
        "3.   Compute $H$\n",
        "4.   If $u < min(1, H)$ then $x_{i+1} = x_*$, else $x_{i+1} = x_i$"
      ],
      "metadata": {
        "id": "6QlyTTJiVJuJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np"
      ],
      "metadata": {
        "id": "otrOzkGKT-Mc"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def mh_sampler(x0, lnprob_fn, prop_fn, prop_fn_kwargs={}, iterations=100000):\n",
        "  \"\"\"\n",
        "  simple MH sampler.\n",
        "\n",
        "  x0: initial array of params\n",
        "  lnprob_fn: fn to compute log-posterior\n",
        "  prop_fn: fn to perform jumps\n",
        "  prop_fn_kwargs: keyword args for proposal fn\n",
        "  iterations: no. of iterations to run sampler\n",
        "\n",
        "  returns: (chain, acceptance, lnprob) tuple of param chain, acceptance rate\n",
        "  and log-posterior chain\n",
        "\n",
        "  note: using log makes computation faster\n",
        "  \"\"\"\n",
        "  # no. of dimensions = no. of params in x0\n",
        "  ndim = len(x0)\n",
        "\n",
        "  # init chain, lnprob and acceptance rate\n",
        "  chain = np.zeros((iterations, ndim)) # rows = iterations, cols = params\n",
        "  lnprob = np.zeros(iterations)\n",
        "  accept_rate = np.zeros(iterations)\n",
        "\n",
        "  # first samples\n",
        "  chain[0] = x0 # first row of chain is whatever x0 will be\n",
        "  lnprob0 = lnprob_fn(x0) # first posterior dist is obtained by inputting first element into lnprob_fn\n",
        "  lnprob[0] = lnprob0 # first element of posterior dist arr is whatever first posterior dist is\n",
        "\n",
        "  # start loop\n",
        "  naccept = 0\n",
        "  for i in range(1, iterations):\n",
        "    # propose\n",
        "    x_star, factor = prop_fn(x0, **prop_fn_kwargs) # factor is new trans prob / first trans prob (proposal ratio)\n",
        "    # draw random uniform no.\n",
        "    u = np.random.uniform(0, 1)\n",
        "    # compute hastings ratio\n",
        "    lnprob_star = lnprob_fn(x_star)\n",
        "    H = np.exp(lnprob_star - lnprob0) * factor # exponent of ln(new post dist/first post dist)\n",
        "    # accept/reject step (update acceptance counter)\n",
        "    if u < H:\n",
        "      x0 = x_star # accepted\n",
        "      lnprob0 = lnprob_star # new posterior dist\n",
        "      naccept += 1\n",
        "    # update chain\n",
        "    chain[i] = x0 # i element in chain takes on x0 value (accepted or otherwise)\n",
        "    lnprob[i] = lnprob0\n",
        "    accept_rate[i] = naccept / i  # no. of acceptance / no. of iterations\n",
        "\n",
        "  return chain, accept_rate, lnprob"
      ],
      "metadata": {
        "id": "cQ_D9GSXmIgV"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "We now need a proposal distribution. Besides drawing from a prior distribution (which would be computationally inefficient), a Gaussian proposal with **fixed** standard deviation is the simplest proposal.\n",
        "\n",
        "Mathematically, it is given by $q(x_*|x_i) = N(x_i, \\sigma^2)$."
      ],
      "metadata": {
        "id": "AyDxGadBXvjP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def gaussian_proposal(x, sigma=0.1): # prop_fn\n",
        "  \"\"\"\n",
        "  gaussian proposal distribution.\n",
        "\n",
        "  draw new params from dist with mean at current position/state x\n",
        "  and a given std dev sigma\n",
        "\n",
        "  proposal is symmetric so ratio of proposal densities is 1.\n",
        "\n",
        "  x: param array\n",
        "  sigma: std dev of gaussian distribution. can be scalar or vector of length(x)\n",
        "  returns: new params, ratio of proposal densities\n",
        "\n",
        "  \"\"\"\n",
        "\n",
        "  # draw x_star\n",
        "  x_star = x + np.random.randn(len(x)) * sigma \n",
        "  # proposal ratio is 1 since jump is symmetric\n",
        "  qxx = 1\n",
        "\n",
        "  return (x_star, qxx)"
      ],
      "metadata": {
        "id": "dajzhSsWT4ZA"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "We now test the sampler and proposal distribution on a simple 1D Gaussian likelihood function with unknown mean and unit variance. We will use a uniform prior on the mean $\\mu ∼ U(-10,10)$.\n",
        "\n",
        "An explanation on continuous uniform distributions can be found [here](https://en.wikipedia.org/wiki/Continuous_uniform_distribution). Essentially, all support values in a distribution have the same probability density."
      ],
      "metadata": {
        "id": "03zw23RsOabY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def simple_gaussian_lnpost(x): # lnprob_fn, this is NOT the \"u\" in the acceptance function\n",
        "  # posterior = prior * likelihood\n",
        "  # derivation can be found: https://www.cs.ubc.ca/~murphyk/Papers/bayesGauss.pdf\n",
        "  \"\"\"\n",
        "  1-D gaussian distribution of means with mu = 0 and std dev = 1.\n",
        "\n",
        "  prior on mean is U(-10, 10)\n",
        "\n",
        "  x: param array\n",
        "  returns: log post\n",
        "  \"\"\"\n",
        "  mu, std = 0, 1\n",
        "  if -10 < x < 10: # if within supports\n",
        "    return -0.5 * (x-mu)**2 / std**2 # apply ln to gaussian posterior dist\n",
        "  else:\n",
        "    return -1e6 # outside of support, posterior dist becomes negative"
      ],
      "metadata": {
        "id": "PunePlPekiJ0"
      },
      "execution_count": 4,
      "outputs": []
    }
  ]
}